import os
import json
from openai import OpenAI
from tqdm import tqdm
from PIL import Image
import numpy as np
from ..utils import get_parser, get_save_path, get_eval_path, get_api_response, encode_image, read_jsonl_file
import time

parser = get_parser()
parser.add_argument("--test_model_name", type=str, default="test_model")
parser.add_argument("--test_prompt_strategy", type=str, default=None)

args = parser.parse_args()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"), 
    base_url=os.getenv("OPENAI_API_BASE"), 
)


# Read the JSONL file
generated_code_file =  get_save_path(args)
content_list = read_jsonl_file(generated_code_file)


if args.test_prompt_strategy is not None:
    test_file = generated_code_file.replace(args.prompt_strategy, args.test_prompt_strategy)
else:
    test_file = generated_code_file.replace(args.model_name, args.test_model_name)

compared_content_list = read_jsonl_file(test_file)

def extract_non_empty_img(content_list):
    non_empty_img = {}
    for item in content_list:
        test_image_path = item['generated_image_path']
        img = Image.open(test_image_path)
        img_np = np.array(img)

        if np.all(img_np == 255):
            continue  # Skip this iteration
        
        idx = int(item['generated_image_path'].rstrip('.png').split('test_image_')[-1])
        non_empty_img[idx] = test_image_path

    return non_empty_img

def extract_ground_truth_img(content_list):
    ground_truth_img = {}
    for item in content_list:
        ground_truth_path = item['ground_truth_path']
        
        idx = int(item['ground_truth_path'].rstrip('.png').split('ground_truth_image_')[-1])
        ground_truth_img[idx] = ground_truth_path

    return ground_truth_img


compare_prompt = "Please act as an impartial judge and evaluate the quality of the generated images provided by two AI assistants given the ground truth image displayed below. " + \
"You should choose the assistant that generate the more similar image. Your evaluation should consider factors such as the overall appearance, colors, shapes, positions, and other visual elements of the images." 

output_prompt = "Begin your evaluation by comparing the two responses and provide a short explanation. Avoid any biases and ensure that the order in which the responses were presented does not influence your decision. " + \
    "Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie."
# Function to evaluate the similarity between two images
def compare_image(gt_path, image_path1, image_path2):
    gt_image = encode_image(gt_path)
    base64_image1 = encode_image(image_path1)
    base64_image2 = encode_image(image_path2)

    messages=[
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": "You are a helpful assistant."
                    }, 
                ]
            }, 
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": compare_prompt
                    },
                    {
                        "type": "text",
                        "text": "Here is the ground truth image."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{gt_image}"
                        },
                    },
                    {
                        "type": "text",
                        "text": "Here is the image generated by the assistant A."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image1}"
                        },
                    },
                    {
                        "type": "text",
                        "text": "Here is the image generated by the assistant B."
                    },
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{base64_image2}"
                        },
                    },
                    {
                        "type": "text",
                        "text": output_prompt
                    },
                ],
            }
        ]
    response = get_api_response(client, messages, args, model_name='gpt-4-vision-preview')
    judgement = response.choices[0].message.content.strip()
    if "[[A]]" in judgement:
        winner = "A"
    elif "[[B]]" in judgement:
        winner = "B"
    elif "[[C]]" in judgement:
        winner = "tie"
    else:
        winner = "error"
    return winner, judgement



img_dict_1 = extract_non_empty_img(content_list)
img_dict_2 = extract_non_empty_img(compared_content_list)
gt_dict = extract_ground_truth_img(content_list)

img_idx_set1 = set(img_dict_1.keys())
img_idx_set2 = set(img_dict_2.keys())

common_img_idx = img_idx_set1.intersection(img_idx_set2)

eval_dir = get_eval_path(args)

if args.test_prompt_strategy is not None:
    args.test_model_name = args.test_model_name + "_" + args.test_prompt_strategy
    
pair_compared_result_file = os.path.join(eval_dir, args.test_model_name + '_compared_results.jsonl')

previous_results = None
evaluated_idx = None

if os.path.exists(pair_compared_result_file) and os.path.getsize(pair_compared_result_file) > 0:
    previous_results = read_jsonl_file(pair_compared_result_file)
    evaluated_idx = [item['question_id'] for item in previous_results]
    
with open(pair_compared_result_file, "w") as results_file:

    if previous_results is not None:
        for result in previous_results:
            results_file.write(json.dumps(result) + "\n")
            results_file.flush()

    for idx in tqdm(common_img_idx):

        if evaluated_idx is not None and idx in evaluated_idx:
            continue

        image_path1 = img_dict_1[idx]
        image_path2 = img_dict_2[idx]
        gt_path = gt_dict[idx]

        round1_winner, round1_judgement = compare_image(gt_path, image_path1, image_path2)
        round2_winner, round2_judgement = compare_image(gt_path, image_path2, image_path1)

        round1_map = {"A": args.model_name, "B": args.test_model_name}
        round2_map = {"A": args.test_model_name, "B": args.model_name}
        round1_winner = round1_map.get(round1_winner, round1_winner)
        round2_winner = round2_map.get(round2_winner, round2_winner)

        result = {
            "question_id": idx,
            "model_1": args.model_name,
            "model_2": args.test_model_name,
            "round1_winner": round1_winner,
            "round2_winner": round2_winner,
            "round1_judgement": round1_judgement,
            "round2_judgement": round2_judgement,
            "tstamp": time.time(),
        }

        results_file.write(json.dumps(result) + "\n")
        results_file.flush()

results = read_jsonl_file(pair_compared_result_file)
print(f"Total number of compared pairs: {len(results)}")

win_cnt = 0
loss_cnt = 0
tie_cnt = 0

for item in results:
    if item["round1_winner"] == "tie" or item["round2_winner"] == "tie" or item["round1_winner"] != item["round2_winner"]:
        tie_cnt += 1
    elif item["round1_winner"] == args.model_name:
        win_cnt += 1
    else:
        loss_cnt += 1
    
print(f"Win: {win_cnt}, Loss: {loss_cnt}, Tie: {tie_cnt}")
print(f"Win Ratio: {win_cnt / len(results)}")
print(f"Tie Ratio: {tie_cnt / len(results)}")
print(f"Loss Ratio: {loss_cnt / len(results)}")
