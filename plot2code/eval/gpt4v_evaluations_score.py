import base64
import os
import openai
import json
import re
from openai import OpenAI
from tqdm import tqdm
from PIL import Image
import numpy as np
from ..utils import get_parser, get_save_path, get_eval_path, get_api_response, read_jsonl_file, encode_image

parser = get_parser()
args = parser.parse_args()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"), 
    base_url=os.getenv("OPENAI_API_BASE"), 
)


# Function to evaluate the similarity between two images
def evaluate_image_similarity(image_path1, image_path2):
    base64_image1 = encode_image(image_path1)
    base64_image2 = encode_image(image_path2)

    messages=[
            {
                "role": "system",
                "content": [
                    {
                        "type": "text",
                        "text": "You are a helpful assistant."
                    }, 
                ]
            }, 
            {
            "role": "user",
            "content": [
                {
                "type": "text",
                "text": "Please evaluate the similarity between a reference image created using matplotlib and an image generated by code provided by an AI assistant. Consider factors such as the overall appearance, colors, shapes, positions, and other visual elements of the images. Begin your evaluation by providing a short explanation. Be as objective as possible. After providing your explanation After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\".",
                },
                {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{base64_image1}"
                },
                },
                {
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/png;base64,{base64_image2}"
                },
                },
            ],
            }
        ]
    response = get_api_response(client, messages, args, model_name='gpt-4-vision-preview')

    return response.choices[0].message.content.strip()

# Read the JSONL file
generated_code_file =  get_save_path(args)

evaluation_file = os.path.join(get_eval_path(args), args.gpt4_vision_evaluation_results)

items = read_jsonl_file(generated_code_file)

# Evaluate the similarity between ground truth images and test images
results = []
total_rating = 0


# check if the evaluation file is empty. If it is not empty, read the file and skip the evaluation for the images that have already been evaluated
if os.path.exists(evaluation_file) and os.path.getsize(evaluation_file) > 0:
    previous_results = read_jsonl_file(evaluation_file)
    evaluated_ground_truth_paths = [item['ground_truth_path'] for item in previous_results]
    total_rating = sum([item['rating'] for item in previous_results if item['rating'] is not None])
    
# Save the evaluation results to a new JSONL file
with open(evaluation_file, "w") as jsonl_file:
    
    if total_rating > 0:
        # Write the previous results to the evaluation file
        for result in previous_results:
            jsonl_file.write(json.dumps(result) + "\n")
            jsonl_file.flush()
    else:
        previous_results = []
        evaluated_ground_truth_paths = []
        
    # Wrap the loop with tqdm to show the progress bar
    for item in tqdm(items, desc="Evaluating image similarity"):
        ground_truth_path = item['ground_truth_path']
        
        if ground_truth_path in evaluated_ground_truth_paths:
            continue # Skip this iteration
        
        test_image_path = item['generated_image_path']
        img = Image.open(test_image_path)
        img_np = np.array(img)
       
        # Check if test_image is all white
        if np.all(img_np == 255):
            print(f"Skipping all white image: {test_image_path}")
            continue  # Skip this iteration
        
        evaluation = evaluate_image_similarity(ground_truth_path, test_image_path)

        # Extract the rating and add it to the total_rating
        rating_match = re.search(r'Rating: \[\[(\d+)\]\]', evaluation)
        if rating_match:
            rating = int(rating_match.group(1))
            total_rating += rating
        else:
            rating = None

        # Update the results dictionary with the new key for the rating
        result = {'ground_truth_path': ground_truth_path, 'test_image_path': test_image_path, 'evaluation': evaluation, 'rating': rating}
        previous_results.append(result)

        jsonl_file.write(json.dumps(result) + "\n")
        jsonl_file.flush()

# Calculate the average rating
average_evaluation_score = np.mean([result['rating'] for result in previous_results if result['rating'] is not None])
print(f"Average Rating: {average_evaluation_score:.2f}")